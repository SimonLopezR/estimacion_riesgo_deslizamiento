# Contextualizaci√≥n de la Problem√°tica 

# Exploraci√≥n de modelos de clasificaci√≥n e implementaci√≥n de calibraci√≥n

## Introducci√≥n

Se emprende un an√°lisis exhaustivo de un problema de clasificaci√≥n, comenzando con un enfoque sistem√°tico para garantizar resultados robustos y confiables. El primer paso en este proceso es realizar un An√°lisis Exploratorio de Datos (EDA), que permitir√° examinar la distribuci√≥n de las variables en el conjunto de datos de entrenamiento. A trav√©s del EDA, se busca identificar patrones subyacentes, y cualquier irregularidad que pueda influir en el rendimiento de los modelos. Esta etapa es crucial, ya que una comprensi√≥n profunda de los datos es fundamental para tomar decisiones informadas en las siguientes fases del an√°lisis.

Una vez extra√≠das las principales caracter√≠sticas del conjunto de datos, se proceder√° a explorar el rendimiento de cuatro modelos de clasificaci√≥n de gran relevancia: Support Vector Machines (SVM), Random Forest Classifier, Gradient Boosting Trees y Regresi√≥n Log√≠stica. Cada uno de estos modelos posee sus propias fortalezas y debilidades, y el objetivo es evaluarlos en un entorno controlado para determinar cu√°l de ellos se adapta mejor al problema en cuesti√≥n. A trav√©s de la comparaci√≥n de m√©tricas clave como la precisi√≥n, la sensibilidad, la especificidad y el AUC-ROC, se buscar√° identificar el modelo que ofrezca el mejor equilibrio entre sesgo y varianza.

Adem√°s de evaluar el rendimiento base de estos modelos, un aspecto crucial del an√°lisis ser√° la calibraci√≥n de probabilidades. Muchos modelos de clasificaci√≥n, especialmente aquellos como SVM y Gradient Boosting Trees, pueden producir predicciones de probabilidad que no est√°n perfectamente calibradas. Esto significa que la probabilidad predicha no siempre refleja adecuadamente la verdadera probabilidad de pertenencia a una clase. Por lo tanto, se examinar√° si los modelos necesitan un ajuste adicional a trav√©s de t√©cnicas de calibraci√≥n, como la regresi√≥n isot√≥nica o el m√©todo de Platt, para mejorar la precisi√≥n de las probabilidades predichas.

Finalmente, el an√°lisis concluir√° con una comparaci√≥n integral de los resultados obtenidos de los modelos, tanto antes como despu√©s de la calibraci√≥n de probabilidades. El objetivo es identificar el modelo que no solo tenga un alto rendimiento en t√©rminos de precisi√≥n, sino que tambi√©n ofrezca predicciones de probabilidad bien calibradas. Este enfoque detallado garantizar√° que el modelo seleccionado no solo sea el m√°s preciso, sino tambi√©n el m√°s fiable para la toma de decisiones en la prediccion del riesgo de deslizamiento de tierra.

## Objetivos

- Desarrollar un modelo de clasificaci√≥n binaria para estimar las clases de deslizamientos de tierra positivas y negativas para un conjunto de datos de entrenamiento dado, mediante diferentes t√©cnicas para mejorar rendimientos de los modelos con el fin de tener un insumo para la calibraci√≥n de las probabilidades 

<br>

- Estimar el m√©todo de calibraci√≥n a usar, por medio de exploraci√≥n de diferentes m√©todos que se ajusten al comportamiento del modelo para estimar la probabilidad de deslizamiento de tierra dada las calcificaciones del modelo entrenado 

<br>

- Desplegar mapa geogr√°fico de la ciudad de Cali para que pueda capturar por zonas geogr√°ficas las probabilidades de deslizamiento de tierras con el fin de tener una herramienta predictiva de anticipaci√≥n 

## Datos

Los deslizamientos de tierra (tambi√©n llamados movimientos en masa o derrumbes) son los eventos de riesgo de mayor frecuencia e impacto en el territorio de Santiago de Cali. Estas situaciones tienen diferentes detonantes como la lluvia acumulada o intensa, intervenci√≥n humana y sismos. El desarrollo de un Mecanismo de Anticipaci√≥n o Alerta Temprana para este tipo de fen√≥meno es esencial para la preparaci√≥n institucional y comunitaria con el objetivo de salvar vidas y bienes en el territorio. 

La alcald√≠a de Cali, al integrarse con empresas expertas en desarrollo tecnol√≥gico y anal√≠tica, pretende levantar este requerimiento de un modelo predictor de deslizamiento de tierras. Junto con SATIC ([Sistema de Alertas Tempranas Inteligentes y Comunitarias](https://satic.cali.gov.co/satic)), las oficinas gestoras de la alcald√≠a y la empresa donde trabajo se han empezado en el desarrollo de este modelo. 

El equipo de ge√≥logos de SATIC, cuenta con los siguientes datos que recibe de diversas fuentes y que ya tienen catalogados como eventos de ‚ÄúS√≠ deslizamiento‚Äù y eventos de ‚ÄúNo deslizamiento‚Äù dado su tiempo enfrentando este tipo de desastres. En el siguiente mapa de ejemplo, se muestran los datos donde por latitud y longitud se ubica cada putno de evento, los puntos rojos indican S√≠ deslizamiento y los verdes indican No deslizamiento. 

![eventos](./resources/puntos_eventos.png)


Importamos los conjuntos de datos de entrenamiento con el que contamos, los cuales pasaron por un pre-procesamiento para tenerlos listos para la anal√≠tica. Estos son provenientes de los archivos Eventos_SI_MM.csv y Eventos_NO_MM.csv proporcionados por la oficina de SATIC de la alcald√≠a de Cali. Estos contienen datos sobre de precipitaciones en diferentes periodos de tiempo, algunas caracter√≠sticas del suelo y la categorizaci√≥n binaria si hubo un derrumbe o no 

A continuaci√≥n, podemos ver la biblioteca de datos de las variables iniciales a tomar en cuenta con su descripci√≥n, cabe resaltar que cada una de estas features est√° asociada a un punto geogr√°fico visto en la imagen anterior 

| **Variable**        | **Tipo de dato** | **Descripci√≥n**                                                                                                                                  |
|---------------------|------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|
| Elevacion           | Num√©rico          | Elevaci√≥n del punto de movimiento en masa en metros sobre el nivel del mar (metros)                                                              |
| Pendiente           | Num√©rico          | Pendiente del punto de movimiento en masa en porcentaje (%)                                                                                      |
| Zonificacion        | Categ√≥rico        | Es un nivel de riesgo: Nombre de la clasificaci√≥n de amenaza o riesgo del punto de movimiento en masa asignado por ge√≥logos de la alcald√≠a de Cali basado en el historial de casos de deslizamientos de tierra en la ciudad |
| Tipo_geologia       | Categ√≥rico        | Nombre de la unidad geol√≥gica asociada al punto de movimiento en masa                                                                             |
| Cobertura_Suelo     | Categ√≥rico        | Nombre de la cobertura de suelo Metodolog√≠a Land Corine Escala 1:25000                                                                            |
| 24h                 | Num√©rico          | Precipitaci√≥n de lluvia acumulada en 24 horas en mil√≠metros. Este dato proviene de estaciones medidoras de lluvia a trav√©s de la ciudad            |
| 7d                  | Num√©rico          | Precipitaci√≥n de lluvia acumulada en 7 d√≠as en mil√≠metros. Este dato proviene de estaciones medidoras de lluvia a trav√©s de la ciudad              |
| 10d                 | Num√©rico          | Precipitaci√≥n de lluvia acumulada en 10 d√≠as en mil√≠metros. Este dato proviene de estaciones medidoras de lluvia a trav√©s de la ciudad             |
| 15d                 | Num√©rico          | Precipitaci√≥n de lluvia acumulada en 15 d√≠as en mil√≠metros. Este dato proviene de estaciones medidoras de lluvia a trav√©s de la ciudad             |
| 30d                 | Num√©rico          | Precipitaci√≥n de lluvia acumulada en 30 d√≠as en mil√≠metros. Este dato proviene de estaciones medidoras de lluvia a trav√©s de la ciudad             |
| 60d                 | Num√©rico          | Precipitaci√≥n de lluvia acumulada en 60 d√≠as en mil√≠metros. Este dato proviene de estaciones medidoras de lluvia a trav√©s de la ciudad             |
| 90d                 | Num√©rico          | Precipitaci√≥n de lluvia acumulada en 90 d√≠as en mil√≠metros. Este dato proviene de estaciones medidoras de lluvia a trav√©s de la ciudad             |
| deslizamiento       | Binario           | Indica si hubo deslizamiento de tierra en el punto de movimiento en masa (1 √≥ 0)                                                                  |

As√≠ se ven las primeras 5 lineas de nuestra data limpia:

| **ELEVACION** | **PENDIENT** | **zonificacion**  | **Nomenclatura_del_Suelo**                                      | **tipo_geologia**               | **24h** | **7d** | **10d** | **15d** | **30d** | **60d** | **90d** | **deslizamientos** |
|---------------|--------------|-------------------|-----------------------------------------------------------------|---------------------------------|--------|-------|--------|--------|--------|--------|--------|------------------|
| 1085.033813   | 36.38882065   | Riesgo alto       | Otras superficies artificiales sin construcci√≥n                  | Formacion Volcanica             | 20.0   | 33.0  | 38.0   | 254.0  | 477.0  | 773.0  | 872.0  | 1                |
| 1027.715698   | 25.13285637   | Amenaza baja      | Bosque mixto denso bajo de tierra firme                          | Formacion Jamundi               | 13.0   | 17.0  | 17.0   | 51.0   | 65.0   | 118.0  | 252.0  | 0                |
| 1173.279907   | 14.10933781   | Amenaza baja      | Pasto cultivado enmalezado                                       | Formacion Jamundi               | 0.0    | 7.2   | 21.1   | 23.1   | 128.3  | 292.7  | 599.4  | 0                |
| 973.211731    | 12.89175129   | Riesgo bajo       | Zonas urbanas continuas                                          | Cono aluvial rio Melendez        | 0.0    | 17.9  | 22.3   | 41.1   | 70.2   | 249.0  | 363.0  | 0                |
| 1062.026245   | 9.081903458   | Amenaza baja      | Bosque mixto fragmentado con pastos y cultivos                   | Cono aluvial 1 rio Pance         | 0.0    | 48.1  | 88.6   | 163.4  | 304.6  | 533.0  | 722.6  | 0                |
| 1184.547241   | 27.58821106   | Riesgo bajo       | Zonas urbanas continuas                                          | Formacion Volcanica             | 0.0    | 0.0   | 1.0    | 47.0   | 47.0   | 99.0   | 170.0  | 0                |

<br>

## Metodolog√≠a

### Modelos

Dentro de los modelos de aprendizaje autom√°tico para problemas de clasificaci√≥n, se ejecutar√°n pruebas de rendimiento para modelos de ensamble como lo son Random Forest y XGBoost, para modelos basados en el hiperplano como el SVM (support vector machines), y modelos de regresi√≥n para clasificaci√≥n como lo es la regresi√≥n log√≠stica


- Bosques Aleatorios (Random Forest)


- XGBoost (Extreme Gradient Boosting Trees)


- Rgresi√≥n Log√≠stica 


- M√°quinas de Vectores de Soporte (SVM)

### Esquema de entrenamiento

Ahora, la idea es lograr desarrollar una **tuber√≠a de datos** tal que cuando lleguen los datos nuevos a estimar, estos pasen por las mismas transformaciones por las que pasaron los datos de entrenamiento. M√°s precisamente hablando, `pipeline` en `scikit-learn` es una herramienta que te permite concatenar varios pasos de procesamiento de datos y modelado en un solo objeto. Esto facilita la construcci√≥n, entrenamiento y evaluaci√≥n de modelos de aprendizaje autom√°tico, ya que puedes encapsular todo el flujo de trabajo en una √∫nica estructura.

![Sci-kit Learn Pipeline](./resources/pipeline-scikitlearn.png)


De la misma manera, se busca optimizar las estimaciones y la calidad de los modelos, por lo que se usa la metodolog√≠a de **Grid search** o **b√∫squeda de malla** para optimizar par√°metros dentro de los modelos. Es una herramienta poderosa para optimizar los par√°metros de un modelo de aprendizaje autom√°tico. Permite definir una cuadr√≠cula de valores para diferentes par√°metros del modelo y busca exhaustivamente la mejor combinaci√≥n de estos parameros, a lo que se le llama hiperparametrizacion.

En t√©rminos m√°s simples, el `GridSearchCV` realiza una b√∫squeda exhaustiva sobre una cuadr√≠cula de valores especificados para los hiperpar√°metros de un estimador. Esto permite encontrar la combinaci√≥n √≥ptima de hiperpar√°metros que maximiza la precisi√≥n o cualquier otra m√©trica de evaluaci√≥n definida.

La idea detr√°s del GridSearchCV es que, en lugar de ajustar manualmente los hiperpar√°metros del modelo y evaluar su rendimiento, el proceso se automatiza y se realiza de manera sistem√°tica. Esto ahorra tiempo y esfuerzo, ya que el GridSearchCV eval√∫a todas las combinaciones posibles de los hiperpar√°metros dentro de la cuadr√≠cula especificada y selecciona la mejor combinaci√≥n seg√∫n la m√©trica de evaluaci√≥n especificada.

![Grid-Search](./resources/Grid_serach.png)

Tercero, se propone usar **validaci√≥n cruzada aleatoria y dividida**. Esto con el proposito de no usar la clasica division unitaria de entrenamiento-pruba, ya que en este tipo de divisi√≥n el entrenamiento o aprendizaje puede quedar sesgado debido a que al dividir aleatoriamente no sabemos con qu√© proporci√≥n de los datos el modelo aprende, por ejemplo, al dividir una sola vez el conjunto de datos nos puede quedar que en el conjunto de test solo haya valores con el label de 0 (no deslizamiento en este caso) y en el entrenamiento solo existan registros del label 1 (deslizamientos), esto puede ocasionar que el modelo no aprenda de la manera m√°s optima.

Con la validaci√≥n cruzada aleatoria y dividida (`shuffle-split`), cada divisi√≥n (split) est√° compuesta de tanto train_size puntos (disyuntos) para el conjunto de entrenamiento y tantos test_size puntos (disjuntos) para el conjunto de prueba, se fijen inicialmente. Esta divisi√≥n se repite n veces, de forma aleatoria. Por ejemplo en la siguiente imagen, para la ejecuci√≥n de 4 iteraciones de divisi√≥n de un conjunto de datos que consta de 50 puntos, con una fracci√≥n de conjunto de entrenamiento de 0.8 y una fracci√≥n de conjunto de prueba de 0.2 puntos cada uno. Esto no debe ser necesariamente igual a la fraccion completa, podemos usar un `train_size` de 0.5 y un `test_size` de 0.1, quiere decir que habr√°n puntos disyuntos que no tomara para entrenarse ni para testear.

![Shuffle-split](./resources/shufflesplit_diagram.png)

### M√©trica de evaluaci√≥n

En el desarrollo de nuestros modelos de clasificaci√≥n binaria para la identificaci√≥n de zonas propensas a deslizamientos de tierra, enfrentamos una decisi√≥n crucial en la selecci√≥n de la m√©trica de rendimiento a optimizar. La esencia de nuestra clasificaci√≥n distingue dos categor√≠as claves: la clase positiva (1), que indica la presencia de un deslizamiento de tierra, y la clase negativa (0), que se√±ala su ausencia. 

![metrics-classification](./resources/matriz_confusion_ejemplo.png)

Si vemos la anterior matriz de confusi√≥n, para nuestro an√°lisis es vital priorizar la reducci√≥n de los falsos positivos (FP - false positives), es decir, las situaciones donde el modelo predice err√≥neamente que habr√° deslizamiento de tierra cuando en realidad no ocurre. La ocurrencia de estos errores podr√≠a tener consecuencias significativas en t√©rminos de seguridad y preparaci√≥n ante desastres naturales ya que en el contexto de este problema es costoso actuar sobre un falso positivo ya que se cuenta recursos limitados para manejar casos positivos, es decir, recursos limitados para manejar desastres de deslizamientos de tierra que hay que saber administrar.

En este mismo orden de ideas, queremos entonces limitar el n√∫mero de falsos positivos, por lo que entonces nos conviene maximizar la `precision` en la fase de entrenamiento

**Precision** mide cu√°ntas de las muestras predichas como positivas son realmente positivas, es decir, precision intenta responder a la siguiente pregunta: ¬øqu√© proporci√≥n de identificaciones positivas fue correcta?

$$ \text{Precision} = \frac{TP}{TP + FP} $$

Precision se utiliza como m√©trica de rendimiento cuando el objetivo es limitar el n√∫mero de falsos positivos

### Calibradores de probabilidad

Por √∫ltimo, se busca indagar si a cada clasificador le es necesario la implementaci√≥n de la **calibraci√≥n de probabilidades** o no, la calibraci√≥n de probabilidades se utiliza para ajustar las probabilidades predichas por un modelo de clasificaci√≥n para que reflejen mejor las probabilidades reales observadas. En un problema de clasificaci√≥n binaria, como lo es el actual, el modelo no solo estima qu√© clase es la m√°s probable, sino tambi√©n asgina una probabilidad asociada a dicha estimaci√≥n. 

Para que un clasificador probabil√≠stico est√© bien calibrado, la confianza asociada a cada predicci√≥n de clase debe reflejar la probabilidad real de que la etiqueta generada sea la correcta. Aterrizando un poco la idea, los clasificadores bien calibrados son aquellos en los que la salida del m√©todo `predict_proba` se puede interpretar directamente como un nivel de confianza. Por ejemplo, un clasificador bien calibrado (binario) debe clasificar las muestras de tal manera que, entre las muestras a las que asign√≥ un valor de `predict_proba` cercano a, digamos 0.8, aproximadamente el 80% pertenezca efectivamente a la clase positiva, es decir, para que un clasificador probabil√≠stico est√© bien calibrado, la confianza asociada a cada predicci√≥n de clase debe reflejar la probabilidad real de que la etiqueta generada sea la correcta.

Lo vemos expresado matem√°ticamente de esta manera: 

$$
P(\hat{Y} = Y \mid \hat{P} = p) = p, \ \forall p \in [0, 1]
$$

Esta formula indica que  indica que un modelo de clasificaci√≥n est√° bien calibrado cuando la probabilidad predicha 
$ \hat{P} = p $ corresponde exactamente con la probabilidad real de que la clase predicha $ \hat{Y} $ sea correcta (es decir, igual a ùëå, la clase verdadera).

### Medidas para evaluar calibraci√≥n

#### Calibration curves

Existen diferentes m√©todos o herramientas por los cuales se puede probar si un clasificador est√° bien calibrado. Primero, se utilizar√° las "**Calibration curves**" o diagramas de fiabilidad, estos miden qu√© tan bien est√°n calibradas las predicciones probabil√≠sticas de un clasificador. Comparan las probabilidades predichas por un clasificador con las frecuencias observadas de los eventos reales.

En la siguiente gr√°fica de ejemplo, podemos ver el eje X (`Mean predicted value`) representa las probabilidades predichas por los modelos, va de 0 a 1, indicando la confianza con la que los modelos predicen la clase positiva. Respecto al eje Y (`Fraction of positives`)  muestra la fracci√≥n de positivos reales, es decir, la proporci√≥n de veces que un evento predicho como probable en realidad ocurre en el conjunto de datos.  

Todas las series ser√°n comparadas con una l√≠nea que representa el caso de calibraci√≥n perfecta. Si un modelo est√° perfectamente calibrado, los puntos de su curva se alinear√°n con esta diagonal, indicando que las probabilidades predichas coinciden con las frecuencias observadas de los eventos reales. Por ejemplo, una predicci√≥n de 0.6 significar√≠a que en el 60% de los casos ese evento ocurre realmente. Entonces si nos enfocamos por ejemplo en el modelo de regresi√≥n log√≠stica (curva azul), la curva azul est√° bastante cerca de la l√≠nea de calibraci√≥n perfecta. Esto indica que las probabilidades predichas por la regresi√≥n log√≠stica est√°n bastante bien calibradas. Para cualquier probabilidad predicha, la fracci√≥n de positivos observados es similar, lo que significa que este modelo tiene buenas predicciones calibradas. 

![calibration-curves](./resources/calibration-curves.png)

#### Brier score y Log Loss

Por otro lado, otro m√©todo o herramienta para probar si un clasificador est√° bien calibrado, son los indicadores de **Brier score** y **Log Loss**. Cuando se trata de evaluar qu√© tan bien calibrado est√° un modelo, es importante utilizar m√©tricas que cuantifiquen la calidad de las probabilidades predichas, ambas m√©tricas penalizan las probabilidades incorrectas y ofrecen una medida de qu√© tan ajustadas est√°n las probabilidades predichas con las clases reales.

El **Brier Score** es una m√©trica que mide la precisi√≥n de las probabilidades predichas por un clasificador. Eval√∫a cu√°n cercanas est√°n las probabilidades predichas a las etiquetas verdaderas, lo que lo convierte en una herramienta √∫til para verificar la calibraci√≥n. Se define como el error cuadr√°tico medio entre las probabilidades predichas y las clases verdaderas. Matem√°ticamente, es:

$$
\text{Brier Score} = \frac{1}{N} \sum_{i=1}^{N} (p_i - y_i)^2
$$

Donde:

- \( N \) es el n√∫mero de muestras.
- \( p_i \) es la probabilidad predicha para la muestra \( i \).
- \( y_i \) es la etiqueta verdadera para la muestra \( i \) (1 si es positiva y 0 si es negativa).

Interpretaci√≥n:

- **Un Brier Score de 0** indica una predicci√≥n perfecta, donde las probabilidades predichas coinciden exactamente con las clases reales.
- **Un Brier Score de 1** es el peor resultado posible, ya que indica una discrepancia completa entre las probabilidades predichas y las etiquetas reales.
- En general, **cuanto menor sea el Brier Score, mejor calibrado estar√° el clasificador**. 

El **Log Loss**, tambi√©n conocido como entrop√≠a cruzada, es otra m√©trica importante para evaluar la calibraci√≥n. Mientras que el Brier Score mide el error cuadr√°tico entre las probabilidades y las etiquetas, el **Log Loss** se enfoca en penalizar fuertemente las predicciones con **alta confianza pero incorrectas**, mientras m√°s cercana sea la probabilidad predicha a 0 o 1 para la clase correcta, menor ser√° el log loss. Se define como:

$$
\text{Log Loss} = - \frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
$$

Donde:
- \( N \) es el n√∫mero total de muestras.
- \( y_i \) es la etiqueta verdadera para la muestra \( i \), 1 para la clase positiva y 0 para la clase negativa.
- \( p_i \) es la probabilidad predicha de la clase positiva para la muestra \( i \).

Interpretaci√≥n:

- El **Log Loss es siempre positivo**, y **cuanto menor sea, mejor calibrado est√° el modelo**.
- El **Log Loss castiga fuertemente** las predicciones que tienen alta confianza pero son incorrectas. Por ejemplo, si el clasificador predice una probabilidad de 0.99 para la clase positiva, pero la etiqueta real es 0, el error ser√° considerablemente mayor que si la predicci√≥n fuera de 0.6.

#### Expected Calibration Error (ECE)

Por √∫ltimo, el Expected Calibration Error (ECE) es una m√©trica utilizada para evaluar la calibraci√≥n de un modelo de clasificaci√≥n, especialmente en modelos de aprendizaje autom√°tico probabil√≠stico.  El ECE eval√∫a la diferencia promedio entre las probabilidades predichas y la frecuencia real de los eventos en una serie de intervalos (bins) de probabilidad. 

Un ECE bajo indica que el modelo est√° bien calibrado, es decir, las probabilidades predichas son confiables. Un valor alto de ECE sugiere que el modelo es "overconfident" (predice probabilidades m√°s altas de las correctas) o "underconfident" (predice probabilidades m√°s bajas). 

Para calcular el Expected Calibration Error (ECE), las predicciones de probabilidad se dividen en intervalos (o *bins*). Luego, se compara la precisi√≥n promedio de cada bin con la probabilidad promedio predicha de ese bin. La f√≥rmula es:

$$
\text{ECE} = \sum_{i=1}^{B} \frac{|B_i|}{n} \cdot |\text{acc}(B_i) - \text{conf}(B_i)|
$$

donde:

- \( B )  es el n√∫mero de intervalos.
- \( B_i \) es el conjunto de ejemplos en el bin \( i \).
- \( |B_i| \) es el n√∫mero de ejemplos en el bin \( i \).
- \( n \) es el total de ejemplos.
- \( acc(B_i) \) es la precisi√≥n de los ejemplos en el bin \( i \).
- \( conf(B_i) \) es la confianza promedio (o probabilidad promedio predicha) en el bin \( i \).


Se expresa entre 0 y 1 y se podr√≠a interpretar de esta manera: 

- ECE entre 0 y 0.1 (0% a 10%): Generalmente se considera que el modelo est√° bien calibrado. Las predicciones de probabilidad son bastante confiables. 

- ECE entre 0.1 y 0.3 (10% a 30%): El modelo tiene una calibraci√≥n moderada. Puede ser necesario ajustar el modelo o aplicar t√©cnicas de calibraci√≥n. 

- ECE por encima de 0.3 (30%): Se considera una mala calibraci√≥n. Las probabilidades predichas no reflejan adecuadamente las verdaderas frecuencias de las clases, y el modelo puede requerir una reevaluaci√≥n o recalibraci√≥n significativa.

Estos ser√°n los m√©todos que se utilizar√°n en este proyecto para saber si un modelo de clasificaci√≥n est√° bien calibrado o no.

### M√©todos de Calibraci√≥n

Una vez validadas las m√©tricas previamente descritas, de acuerdo a estas se determinar√° si el modelo requiere calibraci√≥n. En caso de ser as√≠, se aplicar√°n m√©todos de calibraci√≥n en el post-procesamiento, lo cual significa que se ajusta un modelo de calibraci√≥n a las salidas de un clasificador ya entrenado con el fin de mejorar la distribucion de las probabilidades de la clase positiva, esto es, la confianza de sus probabilidades. Estos m√©todos son ventajosos porque implican un menor costo computacional en comparaci√≥n con los m√©todos de calibraci√≥n aplicados durante el entrenamiento. Adem√°s, son independientes del modelo entrenado y de la complejidad del problema, ya que solo requieren las predicciones del modelo y la distribuci√≥n real de etiquetas.

Estos m√©todos requieren utilizar las **puntuaciones** del clasificador entrenado junto con el conjunto de datos de calibraci√≥n. Por puntuaciones se entiende que estas indican la confianza en sus predicciones de arroja cada clasificador, pero estas puntuaciones no necesariamente se interpretan como probabilidades v√°lidas para interpretar (por ejemplo, en un SVM, estas puntuaciones pueden ser la distancia al hiperplano de decisi√≥n). 

El objetivo de estos m√©todos es estimar o calibrar la probabilidad de la clase positiva (etiquetada en este caso como 1) a partir de las puntuaciones del clasificador entrenado y un conjunto de datos etiquetados que contenga ejemplos para calibrar las salidas del modelo (conjunto de datos de calibraci√≥n) por medio de diferentes t√©cnicas, como las siguientes que usaremos en este estudio:


- Platt Scaling (Ajuste Sigmoidal o Log√≠stico)

El Platt Scaling es un m√©todo utilizado para calibrar las probabilidades de salida de un clasificador binario, especialmente aquellos que producen salidas no calibradas. Este m√©todo ajusta las probabilidades de predicci√≥n de un modelo ya entrenado (puntuaciones) mediante la utilizaci√≥n de una regresi√≥n log√≠stica.

El modelo de regresi√≥n log√≠stica es una forma de modelar la relaci√≥n entre las puntuaciones del clasificador y la probabilidad de que una observaci√≥n pertenezca a la clase positiva. La forma funcional de la regresi√≥n log√≠stica es:

$$ P(y=1 \mid x) = \sigma(Ax + B) $$


- Isotonic Regression (Regresi√≥n Isot√≥nica)


- Beta Calibration


**Anotaci√≥n:**
Para la implementaci√≥n del modelo regresi√≥n log√≠stica, este suele producir probabilidades calibradas de manera natural, ya que est√° dise√±ado para ajustar una probabilidad de pertenencia a la clase positiva basada en los datos de entrenamiento. La salida de un modelo de regresi√≥n log√≠stica ya se interpreta de por si como una probabilidad entre 0 y 1, ya que el modelo aplica una transformaci√≥n sigmoide a su salida lineal.

Sin embargo, en algunos casos, es posible que las probabilidades de un modelo de regresi√≥n log√≠stica no est√©n perfectamente calibradas. Esto puede suceder si:

- Los datos est√°n desbalanceados: Si tienes clases con distribuciones muy desiguales, la regresi√≥n log√≠stica puede producir probabilidades que no reflejan fielmente la distribuci√≥n real de clases.

- El modelo est√° regularizado: En algunos casos, el uso de regularizaci√≥n (como Ridge o Lasso) puede sesgar un poco las probabilidades, especialmente si la regularizaci√≥n es fuerte.

- Distribuci√≥n de datos en validaci√≥n diferente a la de entrenamiento: Si la distribuci√≥n de los datos en el conjunto de validaci√≥n o prueba es distinta a la de entrenamiento, las probabilidades pueden perder calibraci√≥n.

Para verificar la calibraci√≥n de la salida de un modelo de regresi√≥n log√≠stica, se evaluaran las m√©tricas de calibraci√≥n mencionadas anteriormente y se tomar√°n las decisiones pertinentes

<br>

## **Tabla de contenidos**

```{tableofcontents}
```
